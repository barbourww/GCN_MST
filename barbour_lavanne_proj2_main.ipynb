{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Network for Estimation of Minimum Spanning Tree value\n",
    "\n",
    "We use code from Thomas Kipf and Max Welling available at https://github.com/tkipf/gcn for the graph convolutional network and Tensorflow for the neural network backend.\n",
    "\n",
    "The network is trained on examples of graphs and their corresponding minimum spanning tree (MST) values and can accurately predict, in the testing phase, the MST value for previously unseen graphs of similar size and the same fundamental structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python 3 compatibility\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# neural network and graph convolutional NN packages\n",
    "import tensorflow as tf\n",
    "from gcn.utils import *\n",
    "from gcn.models import Model, MLP\n",
    "from gcn.layers import *\n",
    "\n",
    "# for turning real graphs into features\n",
    "import networkx as nx\n",
    "from scipy.sparse import csc_matrix, csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "# plotting (inline in notebook)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph convolution layer and GCN class declarations\n",
    "Includes loss and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Redefine GraphConvolution layer and GCN class\n",
    "# Changes: \n",
    "#     - added capability to GraphConvolution layer to selectively use weighted adjacency\n",
    "#     - brought loss and accuracy metrics into GCN class\n",
    "#     - added capability to GCN class to build layers according to argument specification\n",
    "# --------------------------------\n",
    "\n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, adjacency='B',\n",
    "                 dropout=False, sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor for graph convolution neural network layer, which uses graph structure,\n",
    "            via support matrix, to enhance connectivity of NN nodes.\n",
    "        :param input_dim: layer input dimension\n",
    "        :param output_dim: layer output dimension\n",
    "        :param placeholders: needed for dropout probability and supports (weighted and ones)\n",
    "        :param dropout: T/F to activate dropout (prob. = placeholders['dropout']) on layer\n",
    "        :param act: activation function\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "        \n",
    "        self.A = 'a' in adjacency.lower()\n",
    "        self.B = 'b' in adjacency.lower()\n",
    "        assert self.A or self.B, \"Adjacency must use any/all of A or B matrices\"\n",
    "        \n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.support_ones = placeholders['support_ones']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_A_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_A_' + str(i))\n",
    "                self.vars['weights_B_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_B_' + str(i))\n",
    "                self.vars['weights_tilde_' + str(i)] = glorot([1, output_dim],\n",
    "                                                              name='weights_tilde_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            # keep probability is (1 - self.dropout)\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup_A = dot(x, self.vars['weights_A_' + str(i)], sparse=self.sparse_inputs)\n",
    "                pre_sup_B = dot(x, self.vars['weights_B_' + str(i)], sparse=self.sparse_inputs)\n",
    "            else:\n",
    "                pre_sup_A = self.vars['weights_A_' + str(i)]\n",
    "                pre_sup_B = self.vars['weights_B_' + str(i)]\n",
    "            if self.B:\n",
    "                support = dot(self.support[i], pre_sup_B, sparse=True)\n",
    "                if self.A:\n",
    "                    support += dot(self.support_ones[i], pre_sup_A, sparse=True)\n",
    "                    support /= 2.\n",
    "            elif self.A:\n",
    "                support = dot(self.support_ones[i], pre_sup_A, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    \"\"\"\n",
    "    Graph convolutional neural network class\n",
    "    \"\"\"\n",
    "    def __init__(self, placeholders, input_dim, layer_config, dropouts, learning_rate, weight_decay, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor for graph convolutional neural network, defined by the layer_config.\n",
    "        :param placeholders: contains features, labels, masks, supports, and dropout probability.\n",
    "        :param input_dim: neural network input dimension (= number of graph nodes)\n",
    "        :param layer_config: list of tuples (layer type, dimension, support type) that define NN structure\n",
    "        :param dropouts: list of T/F that corresponds to NN layers and turns on dropout\n",
    "        :param learning_rate: optimizer learning rate\n",
    "        :param weight_decay: decay applied to L2 loss\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # for regression change output dimension to 1\n",
    "        self.output_dim = 1\n",
    "        \n",
    "        self.layer_config = layer_config\n",
    "        self.dropouts = dropouts\n",
    "        assert len(self.layer_config) == len(self.dropouts), \"layer_config and dropouts must align in length\"\n",
    "        self.placeholders = placeholders\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "        self.build()\n",
    "    \n",
    "    @staticmethod\n",
    "    def masked_mean_square_error(preds, labels, mask):\n",
    "        \"\"\"\n",
    "        Calculate MSE between predictions and labels.\n",
    "        \"\"\"\n",
    "        # take square of difference between predictions and labels\n",
    "        loss = tf.squared_difference(preds, labels)\n",
    "        # sum differences within each observation\n",
    "        loss = tf.reduce_sum(loss, 1)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        # apply mask\n",
    "        loss *= mask\n",
    "        # report mean loss per observation\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def masked_accuracy(preds, labels, mask):\n",
    "        \"\"\"\n",
    "        Calculate MAE between predictions and labels.\n",
    "        \"\"\"\n",
    "        # take absolute of difference between predictions and labels\n",
    "        loss = tf.sqrt(tf.squared_difference(preds, labels))\n",
    "        # sum differences within each observation\n",
    "        loss = tf.reduce_sum(loss, 1)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        # apply mask\n",
    "        loss *= mask\n",
    "        # report mean loss per observation\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += self.weight_decay * tf.nn.l2_loss(var)\n",
    "        self.loss += GCN.masked_mean_square_error(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        # reported accuracy\n",
    "        self.accuracy = GCN.masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                            self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "        \"\"\"\n",
    "        Construct the NN according to the layer_config defined for the class.\n",
    "        \"\"\"\n",
    "        for (lt, ls, lo), d, li in zip(self.layer_config, self.dropouts, range(len(self.layer_config))):\n",
    "            if li == 0:\n",
    "                in_dim = self.input_dim\n",
    "                si = True\n",
    "            else:\n",
    "                in_dim = self.layer_config[li-1][1]\n",
    "                si = False\n",
    "            if li == len(self.layer_config):\n",
    "                out_dim = self.output_dim\n",
    "            else:\n",
    "                out_dim = ls\n",
    "            \n",
    "            if lt.lower() in (\"gc\", \"graph\", \"graphconvolution\"):\n",
    "                self.layers.append(GraphConvolution(input_dim=in_dim, output_dim=out_dim,\n",
    "                                                    placeholders=self.placeholders, adjacency=lo, \n",
    "                                                    act=tf.nn.relu, dropout=d, sparse_inputs=si, \n",
    "                                                    logging=self.logging))\n",
    "            elif lt.lower() in ('d', 'dense'):\n",
    "                self.layers.append(Dense(input_dim=in_dim, output_dim=out_dim,\n",
    "                                         placeholders=self.placeholders, act=tf.nn.relu,\n",
    "                                         dropout=d, sparse_inputs=si, logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Return NN outputs directly (no softmax).\n",
    "        \"\"\"\n",
    "        # pass outputs directly back, since this is set up for regression\n",
    "        return self.outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New class to generate data and access/train GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# New MST_GCN class to leverage GCN to predict value of minimum spanning tree (MST)\n",
    "# --------------------------------\n",
    "\n",
    "class MST_GCN:\n",
    "    \"\"\"\n",
    "    MST_GCN uses a graph convolutional neural network to predict the value of \n",
    "    a graph minimum spanning tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.5, epochs=200, learning_rate=0.01, \n",
    "                 weight_decay=5e-4, early_stopping=100, log=False, logstring=''):\n",
    "        \"\"\"\n",
    "        Class need only be constructed once. Hyperparameters may be updated, \n",
    "        data may be regenerated, and model may be retrained using a single instance.\n",
    "        :param dropout: probability of NN node not being activated\n",
    "        :param epochs: limit of training steps for convergence\n",
    "        :param learning_rate: optimizer learning rate\n",
    "        :param weight_decay: decay applied to L2 loss\n",
    "        :param early_stopping: minimum number of epochs reached before early stop is possible\n",
    "        :param log: T/F to turn on logging of results for all training performed\n",
    "        :param logstring: additional information on log header for record keeping\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # Set random seed\n",
    "        seed = 123\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "        \n",
    "        # Set class initialized log file name\n",
    "        if log:\n",
    "            if 'logs' not in os.listdir('./'):\n",
    "                os.mkdir('./logs')\n",
    "            self.logfn = \"./logs/gcn_trial_{}.txt\".format(datetime.now().strftime(\"%m-%d-%y_%H%M%S\"))\n",
    "            self.csvfn = os.path.splitext(self.logfn)[0] + '.csv'\n",
    "            with open(self.logfn, 'w') as f:\n",
    "                f.write(\"MST GCN trial: {}\\n\".format(self.logfn))\n",
    "                f.write(logstring + '\\n')\n",
    "            with open(self.csvfn, 'w') as f:\n",
    "                f.write(\"MST GCN trial: {}\\n\".format(self.logfn))\n",
    "                f.write(logstring + '\\n')\n",
    "        else:\n",
    "            self.logfn = None\n",
    "            self.csvfn = None\n",
    "        \n",
    "        # modeling/optimization settings        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.dropout = dropout\n",
    "        self.weight_decay = weight_decay\n",
    "        self.early_stopping = early_stopping\n",
    "        \n",
    "        # allocations of training, validation, and testing data\n",
    "        self.val_range = None\n",
    "        self.train_range = None\n",
    "        self.test_range = None\n",
    "        \n",
    "        self.n_nodes = None\n",
    "        self.beta = None\n",
    "        \n",
    "        # masks (trivial as implemented)\n",
    "        self.train_mask = None\n",
    "        self.val_mask = None\n",
    "        self.test_mask = None\n",
    "        \n",
    "        # data for modelling\n",
    "        self.graphs = None\n",
    "        self.mst_vals = None\n",
    "        self.adjs = None\n",
    "        self.y_values = None\n",
    "        self.featureset = None\n",
    "        self.features = None\n",
    "        self.feature_length = None\n",
    "        self.supports = None\n",
    "        self.supports_ones = None\n",
    "        self.num_supports = None\n",
    "        self.placeholders = None\n",
    "        \n",
    "        # modelling results\n",
    "        self.train_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.validation_accuracy = []\n",
    "        self.test_cost = []\n",
    "        self.test_acc = []\n",
    "        self.test_pct_acc = []\n",
    "    \n",
    "    def update_params(self, dropout=0.5, learning_rate=0.01, weight_decay=5e-4):\n",
    "        \"\"\"\n",
    "        Update parameters of class for GCN construction.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_graph(n, beta):\n",
    "        \"\"\"\n",
    "        Make one Waxman graph.\n",
    "        :param n: number of nodes in the graph\n",
    "        :return: graph, value of MST\n",
    "        \"\"\"\n",
    "        # random Waxman graph from NetworkX\n",
    "        g = nx.waxman_graph(n, beta)\n",
    "        n = {i: d for i, d in g.nodes(data=True)}\n",
    "        for node, p in g.nodes(data=True):\n",
    "            if g.degree(node) > 0:\n",
    "                continue\n",
    "            else:\n",
    "                d = [np.power(np.sum(np.power(np.array(p['pos']) - np.array(d['pos']), 2)), 0.5) if u != node else 1000.\n",
    "                     for u, d in g.nodes(data=True)]\n",
    "                neighbor = np.argmin(np.array(d))\n",
    "                g.add_edge(node, neighbor)\n",
    "        while not nx.is_connected(g):\n",
    "            c = sorted(list(nx.connected_components(g)), key=lambda x: len(x))\n",
    "            s = c[0]\n",
    "            mn = (None, None, 10000.)\n",
    "            for sn in c[1]:\n",
    "                d = [(np.power(np.sum(np.power(np.array(g.node[sn]['pos']) - np.array(g.node[u]['pos']), 2)), 0.5), u)\n",
    "                     for u in s]\n",
    "                d.sort(key=lambda x: x[0])\n",
    "                if d[0][0] < mn[2]:\n",
    "                    mn = (sn, d[0][1], d[0][0])\n",
    "            g.add_edge(mn[0], mn[1])\n",
    "        # assign weight to each edge by its length (L2 distance between nodes)\n",
    "        for u, v, d in g.edges(data=True):\n",
    "            d['weight'] = np.power(np.sum(np.power(np.array(n[u]['pos']) - np.array(n[v]['pos']), 2)), 0.5)\n",
    "        # calculate value of weighted minimum spanning tree\n",
    "        m = nx.minimum_spanning_tree(g, weight='weight')\n",
    "        mv = sum([d['weight'] for u, v, d in m.edges(data=True)])\n",
    "        return g, mv\n",
    "\n",
    "    def generate_graphs(self, n_graphs, n_nodes, beta):\n",
    "        \"\"\"\n",
    "        Generate a bunch of Waxman graphs and calculate minimum spanning tree values, \n",
    "            weighted adjacency matrices, and unweighted adjacency matrices.\n",
    "        :param n_graphs: number of graphs\n",
    "        :param n_nodes: number of nodes in each graph\n",
    "        :return: list of graphs, list of MST values, list of w. adj., list of u.w. adj.\n",
    "        \"\"\"\n",
    "        if type(n_nodes) in (int, float):\n",
    "            sizes = [n_nodes]*n_graphs\n",
    "        elif type(n_nodes) in (list, tuple) and len(n_nodes) == n_graphs:\n",
    "            sizes = n_nodes\n",
    "        else:\n",
    "            raise ArgumentError(\"Problem with n_nodes argument.\")\n",
    "        # make a bunch of Waxman graphs\n",
    "        graphs, mst_vals = zip(*[MST_GCN.generate_graph(n=s, beta=beta) for _, s in zip(range(n_graphs), sizes)])\n",
    "        # compute weighted adjacency matrices\n",
    "        adjs = [csr_matrix(np.lib.pad(nx.adjacency_matrix(g, weight='weight').toarray(), \n",
    "                                      ((0, self.n_nodes - nx.number_of_nodes(g)), (0, self.n_nodes - nx.number_of_nodes(g))), \n",
    "                                      'constant', constant_values=0)) for g in graphs]\n",
    "        # compute unweighted adjacency matrices\n",
    "        adjs_ones = [csr_matrix(np.lib.pad(nx.adjacency_matrix(g, weight=None).toarray(), \n",
    "                                           ((0, self.n_nodes - nx.number_of_nodes(g)), (0, self.n_nodes - nx.number_of_nodes(g))), \n",
    "                                           'constant', constant_values=0)) for g in graphs]\n",
    "        return graphs, mst_vals, adjs, adjs_ones\n",
    "\n",
    "    def _evaluate(self, features, support, support_ones, labels, mask, placeholders):\n",
    "        \"\"\"\n",
    "        Model evaluation function for validation and testing of NN.\n",
    "        :return: loss, accuracy, evaluation time elapsed\n",
    "        \"\"\"\n",
    "        # take down start time\n",
    "        t_test = time.time()\n",
    "        # construct feed dictionary with new values\n",
    "        feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "        # update with dropout probability and unweighted support matrix\n",
    "        feed_dict_val.update({self.placeholders['dropout']: self.dropout})\n",
    "        feed_dict_val.update({self.placeholders['support_ones'][i]: support_ones[i] \n",
    "                              for i in range(len(support_ones))})\n",
    "        # run through NN\n",
    "        outs_val = self.sess.run([self.model.loss, self.model.accuracy], feed_dict=feed_dict_val)\n",
    "        return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _normalize_adj(adj, sparse):\n",
    "        \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "        if sparse:\n",
    "            adj = sp.coo_matrix(adj)\n",
    "        rowsum = np.array(adj.sum(1))\n",
    "        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt, 0)\n",
    "        return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _preprocess_adj(adj, neighbor_degree, sparse=True):\n",
    "        \"\"\"\n",
    "        Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\n",
    "        :return: preprocessed adjacency matrix as tuple (from sparse matrix)\n",
    "        \"\"\"\n",
    "        if neighbor_degree == 1:\n",
    "            # first-degree neighbor adjacency A^1\n",
    "            adj_normalized = MST_GCN._normalize_adj(adj + sp.eye(adj.shape[0]), sparse=True)\n",
    "            return sparse_to_tuple(adj_normalized)\n",
    "        elif neighbor_degree == 2:\n",
    "            # second-degree neighbor adjacency A^2\n",
    "            adj_square = np.power(adj, 2).tocoo()\n",
    "            return sparse_to_tuple(adj_square)\n",
    "        else:\n",
    "            raise NotImplementedError(\"neighbor_degree > 2 not implemented at this time\")\n",
    "    \n",
    "    def generate_data(self, n_graphs, n_nodes, train_frac, val_frac, beta, test_n_nodes=None):\n",
    "        \"\"\"\n",
    "        Generate the graph data and split into training, validation, and testing.\n",
    "            Graphs remaining after training and validation allocation are used for testing.\n",
    "        :param n_graphs: number of graphs\n",
    "        :param n_nodes: number of nodes in each graph\n",
    "        :param train_frac: fraction of graphs to use for training\n",
    "        :param val_frac: fraction of graphs to use for validation\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"Generating data.\")\n",
    "        # make sure new data clears features\n",
    "        self.featureset = None\n",
    "        self.features = None\n",
    "        self.n_nodes = n_nodes\n",
    "        # calculate indices for ranges\n",
    "        # right now validation is first range because of random seed in NetworkX\n",
    "        assert train_frac + val_frac <= 1\n",
    "        val_idx = int(n_graphs*val_frac)-1\n",
    "        train_idx = int(n_graphs*(train_frac + val_frac))-1\n",
    "        # create the three non-overlapping ranges of indices for training, validation, and testing\n",
    "        self.val_range = range(0, val_idx)\n",
    "        self.train_range = range(val_idx, train_idx)\n",
    "        self.test_range = range(train_idx, n_graphs-1)\n",
    "        # masks are [1] because only one graph is passed at a time to train/validate/test (correct to type)\n",
    "        self.train_mask = np.array([1])\n",
    "        self.val_mask = np.array([1])\n",
    "        self.test_mask = np.array([1])\n",
    "        #\n",
    "        if test_n_nodes is None or test_n_nodes == n_nodes:\n",
    "            pass\n",
    "        elif test_n_nodes < n_nodes:\n",
    "            n_nodes = [n_nodes if i < int(n_graphs*(train_frac + val_frac))-1 else test_n_nodes for i in range(n_graphs)]\n",
    "        else:\n",
    "            raise ArgumentError(\"test_n_nodes > n_nodes\")\n",
    "        # generate graphs\n",
    "        self.graphs, self.mst_vals, self.adjs, self.adjs_ones = self.generate_graphs(n_graphs=n_graphs, n_nodes=n_nodes, beta=beta)\n",
    "        # make each y-value an array - will only train/validate/test on one at a time\n",
    "        self.y_values = np.array([np.array([l]).reshape(1, 1) for l in self.mst_vals])\n",
    "    \n",
    "    def generate_features(self, feature_gen):\n",
    "        \"\"\"\n",
    "        Generate specified feature vectors for nodes in each graph.\n",
    "            Available features: 'degree', 'weighted_degree', 'weighted_second_degree', 'adj', 'path'\n",
    "        :param feature_gen: list of features to generate for each node of each graph\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"Generating features.\")\n",
    "        self.featureset = feature_gen\n",
    "        features = []\n",
    "        for g in self.graphs:\n",
    "            vecs = []\n",
    "            if 'degree' in feature_gen:\n",
    "                # unweighted degree of on-hop neighborhood\n",
    "                degrees = [g.degree(n, weight=None) for n in g.nodes()]\n",
    "                d_vec = ((np.array(degrees) - min(degrees)) / (max(degrees) - min(degrees))).reshape(len(degrees), 1)\n",
    "                vecs.append(d_vec)\n",
    "            if 'weighted_degree' in feature_gen:\n",
    "                # weighted degree of one-hop neighborhood\n",
    "                w_degrees = [g.degree(n, weight='weight') for n in g.nodes()]\n",
    "                wd_vec = ((np.array(w_degrees) - min(w_degrees)) / (max(w_degrees) - min(w_degrees))).reshape(len(w_degrees), 1)\n",
    "                vecs.append(wd_vec)\n",
    "            if 'weighted_second_degree' in feature_gen:\n",
    "                # weighted degree of two-hop neighborhood\n",
    "                w2_degrees = [sum([g.degree(n2, weight='weight') for n2 in g.neighbors(n) if n2 != n]) for n in g.nodes()]\n",
    "                wd2_vec = ((np.array(w2_degrees) - min(w2_degrees)) / (max(w2_degrees) - min(w2_degrees))).reshape(len(w2_degrees), 1)\n",
    "                vecs.append(wd2_vec)\n",
    "            if 'adj' in feature_gen:\n",
    "                padn = self.n_nodes - nx.number_of_nodes(g)\n",
    "                adj_vecs = list(np.lib.pad(nx.adjacency_matrix(g, weight=None).toarray(), ((0, padn), (0, padn)), \n",
    "                                           'constant', constant_values=0))\n",
    "                vecs.extend([av.reshape(len(av), 1) for av in adj_vecs])\n",
    "            if 'path' in feature_gen:\n",
    "                # define the feature matrix as the weighted shortest path length between nodes\n",
    "                #####\n",
    "                # TODO: get this to the proper size for undersized test graphs\n",
    "                #####\n",
    "                paths = list(np.array([[nx.single_source_dijkstra_path_length(g, n1, weight='weight').get(n2, 0) \n",
    "                                        for n2 in g.nodes()] for n1 in g.nodes()]))\n",
    "                vecs.extend([p.reshape(len(p), 1) for p in paths])\n",
    "            \n",
    "            new_vecs = []\n",
    "            for fv in vecs:\n",
    "                if fv.shape[0] < self.n_nodes:\n",
    "                    new_vecs.append(np.lib.pad(fv, ((0,int(self.n_nodes - len(fv))), (0,0)), 'constant', constant_values=0))\n",
    "                else:\n",
    "                    new_vecs.append(fv)\n",
    "            # concatenate feature vectors, which should each be of length n_nodes\n",
    "            f_vec = np.concatenate(new_vecs, axis=1)\n",
    "            # get correct second dimension\n",
    "            try:\n",
    "                dim2 = f_vec.shape[1]\n",
    "            except IndexError:\n",
    "                # when only one feature is generated\n",
    "                dim2 = 1\n",
    "            # change feature matrix to sparse matrix for compatibility with GCN preprocess_features(.) function\n",
    "            features.append(csc_matrix(f_vec.reshape(f_vec.shape[0], dim2)))\n",
    "        # feature preprocessing (GCN utility)\n",
    "        self.features = [preprocess_features(f) for f in features]\n",
    "    \n",
    "    def generate_supports(self, neighbor_degree=1):\n",
    "        \"\"\"\n",
    "        Generate the support matrices for each graph, weighted and unweighted.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # adjacency matrices with edge weight\n",
    "        self.supports = [[MST_GCN._preprocess_adj(b, neighbor_degree=neighbor_degree)] for b in self.adjs]\n",
    "        # adjacency matrices without edge weights (zeros and ones)\n",
    "        self.supports_ones = [[MST_GCN._preprocess_adj(a, neighbor_degree=neighbor_degree)] for a in self.adjs_ones]\n",
    "        # hard coded because ability to generate multiple supports is not implemented\n",
    "        self.num_supports = 1\n",
    "        # Define placeholders\n",
    "        self.placeholders = {\n",
    "            'support': [tf.sparse_placeholder(tf.float32) for _ in range(self.num_supports)],\n",
    "            'support_ones': [tf.sparse_placeholder(tf.float32) for _ in range(self.num_supports)],\n",
    "            # use the first feature vector (sparse matrix representation) at index [0] and get the shape at index [2]\n",
    "            'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(self.features[0][2], dtype=tf.int64)),\n",
    "            'labels': tf.placeholder(tf.float32, shape=(None, 1)),\n",
    "            'labels_mask': tf.placeholder(tf.int32),\n",
    "            'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "            'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "        }\n",
    "\n",
    "    def train_model(self, n_graphs, n_nodes, train_frac, val_frac, layer_config, dropouts,\n",
    "                    feature_gen=['weighted_degree'], beta=0.8, test_n_nodes=None, \n",
    "                    force_regen_data=False, force_regen_supports=False, verbose=False):\n",
    "        \"\"\"\n",
    "        Build specified NN and perform model training, validation, and testing.\n",
    "            Graphs not allocated to training or validation are used for testing.\n",
    "        :param n_graphs: number of graphs\n",
    "        :param n_nodes: number of nodes in each graph\n",
    "        :param train_frac: fraction of graphs used for training\n",
    "        :param val_frac: fraction of graphs sued for validation\n",
    "        :param layer_config: list of tuples (layer type, size, support) specifying NN structure\n",
    "        :param dropouts: list of T/F aligning to layers that turn on/off dropout by layer\n",
    "        :param feature_gen: list of features to generate for each node in each graph\n",
    "        :param force_regen_data: T/F force regenerate data\n",
    "        :param force_regen_support: T/F force regenerate supports\n",
    "        :param verbose: print NN training progress\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        \n",
    "        # make sure layer configuration and dropout configuration are of valid types\n",
    "        try:\n",
    "            s1 = \"layer_config should be list of tuples of form (layer type, layer size)\"\n",
    "            assert all([type(lt) is str and type(ls) is int and type(lo) is str for lt, ls, lo in layer_config]), s1\n",
    "            s2 = \"dropouts should be iterable, corresponding to each layer\"\n",
    "            assert all([d in (True, False) for d in dropouts]) and len(dropouts) == len(layer_config), s2\n",
    "        except BaseException as e:\n",
    "            # to catch exception when layer_config is not iterable of iterables (each of size 2)\n",
    "            # or dropouts is not iterable\n",
    "            print(\"layer_config should be list of tuples of form (layer type, layer size)\")\n",
    "            raise e\n",
    "        \n",
    "        # only generate data if not in place or if forced to regenerate\n",
    "        regen_data = self.y_values is None or self.n_nodes != n_nodes or (self.n_nodes != test_n_nodes and test_n_nodes is not None) or self.beta != beta or force_regen_data\n",
    "        if regen_data:\n",
    "            self.beta = beta\n",
    "            self.generate_data(n_graphs, n_nodes, train_frac, val_frac, beta=beta, test_n_nodes=test_n_nodes)\n",
    "        \n",
    "        # only regenerate features when a new featureset is specified or when data is regenerated\n",
    "        if self.featureset != feature_gen or self.features is None or regen_data:\n",
    "            self.generate_features(feature_gen=feature_gen)\n",
    "        \n",
    "        # only generate supports if not in place or if forced to regenerate (data or supports)\n",
    "        if self.supports is None or regen_data or force_regen_supports:\n",
    "            self.generate_supports()\n",
    "        \n",
    "        # Create model\n",
    "        # send layer configuration and dropouts specific to this model training run\n",
    "        # use first feature vector at index [0] and get the shape at index [2] and the column dimension of the shape\n",
    "        self.model = GCN(placeholders=self.placeholders, input_dim=self.features[0][2][1], \n",
    "                         layer_config=layer_config, dropouts=dropouts,\n",
    "                         learning_rate=self.learning_rate, weight_decay=self.weight_decay, logging=True)\n",
    "        \n",
    "        # Initialize session\n",
    "        self.sess = tf.Session()\n",
    "        # Initialize weights\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.validation_accuracy = []\n",
    "        self.test_cost = []\n",
    "        self.test_acc = []\n",
    "        self.test_pct_acc = []\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting optimization...\")\n",
    "        for epoch in range(self.epochs):\n",
    "            # TRAINING PHASE for epoch\n",
    "            epoch_outs = []\n",
    "            for graph_i in self.train_range:\n",
    "                t = time.time()\n",
    "                # Construct feed dictionary\n",
    "                feed_dict = construct_feed_dict(self.features[graph_i], self.supports[graph_i], \n",
    "                                                self.y_values[graph_i], self.train_mask, self.placeholders)\n",
    "                # update with dropout probability and unweighted support matrix\n",
    "                feed_dict.update({self.placeholders['dropout']: self.dropout})\n",
    "                feed_dict.update({self.placeholders['support_ones'][i]: self.supports_ones[graph_i][i] \n",
    "                                  for i in range(len(self.supports_ones[graph_i]))})\n",
    "                # NN training\n",
    "                outs = self.sess.run([self.model.opt_op, self.model.loss, self.model.accuracy], feed_dict=feed_dict)\n",
    "                epoch_outs.append(outs)\n",
    "            \n",
    "            # VALIDATION PHASE for epoch\n",
    "            epoch_costs = []\n",
    "            epoch_acc = []\n",
    "            epoch_dur = []\n",
    "            for graph_i in self.val_range:\n",
    "                # Validation\n",
    "                cost, acc, duration = self._evaluate(self.features[graph_i], self.supports[graph_i], self.supports_ones[graph_i],\n",
    "                                                     self.y_values[graph_i], self.val_mask, self.placeholders)\n",
    "                epoch_costs.append(cost)\n",
    "                epoch_acc.append(acc)\n",
    "                epoch_dur.append(duration)\n",
    "            \n",
    "            self.validation_loss.append(sum(epoch_costs)/len(epoch_costs))\n",
    "            self.validation_accuracy.append(sum(epoch_acc)/len(epoch_acc))\n",
    "            self.train_loss.append(sum([o[1] for o in epoch_outs])/len(epoch_outs))\n",
    "            self.train_accuracy.append(sum([o[2] for o in epoch_outs])/len(epoch_outs))\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Epoch:\", '%04d' % (epoch + 1), \n",
    "                      \"train_loss=\", \"{:.5f}\".format(self.train_loss[-1]),\n",
    "                      \"train_acc=\", \"{:.5f}\".format(self.train_accuracy[-1]), \n",
    "                      \"val_loss=\", \"{:.5f}\".format(self.validation_loss[-1]),\n",
    "                      \"val_acc=\", \"{:.5f}\".format(self.validation_accuracy[-1]), \n",
    "                      \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "            if epoch > self.early_stopping and self.validation_loss[-1] > np.mean(self.validation_loss[-(self.early_stopping+1):-1]):\n",
    "                print(\"Early stopping.\")\n",
    "                r = range(-(self.early_stopping+1), -1)\n",
    "                break\n",
    "        else:\n",
    "            # didn't early stop\n",
    "            r = [-1]\n",
    "        print(\"Optimization Finished!\")\n",
    "        \n",
    "        # TESTING PHASE\n",
    "        for graph_i in self.test_range:\n",
    "            tc, ta, test_duration = self._evaluate(self.features[graph_i], self.supports[graph_i], self.supports_ones[graph_i], \n",
    "                                                   self.y_values[graph_i], self.test_mask, self.placeholders)\n",
    "            self.test_cost.append(tc)\n",
    "            self.test_acc.append(ta)\n",
    "            self.test_pct_acc.append(100*ta/self.y_values[graph_i].flatten()[0])\n",
    "            if verbose:\n",
    "                print(\"Test set results:\", \"mst=\", \n",
    "                      self.y_values[graph_i], \"cost=\", \n",
    "                      \"{:.5f}\".format(self.test_cost[-1]),\n",
    "                      \"accuracy=\", \"{:.5f}\".format(self.test_acc[-1]),\n",
    "                      \"pct_accuracy=\", \"{:.2f}\".format(self.test_pct_acc[-1]),\n",
    "                      \"time=\", \"{:.5f}\".format(test_duration))\n",
    "        \n",
    "        # list of outputs to send to log or console\n",
    "        prints = [\"\\n-- Model parameters --\",\n",
    "                  \"Layer configuration: {}\".format(layer_config),\n",
    "                  \"Dropout configuration: {}\".format(dropouts),\n",
    "                  \"Features: {}\".format(feature_gen),\n",
    "                  \"Dropout rate: {}\".format(self.dropout),\n",
    "                  \"Learning rate: {}\".format(self.learning_rate),\n",
    "                  \"Weight decay: {}\".format(self.weight_decay),\n",
    "                  \"Beta: {}\".format(self.beta),\n",
    "                  \n",
    "                  \"-- Model results --\",\n",
    "                  \"mean training loss: {}\".format(np.mean([self.train_loss[l] for l in r])),\n",
    "                  \"mean training accuracy: {}\".format(np.mean([self.train_accuracy[l] for l in r])),\n",
    "                  \"mean validataion loss: {}\".format(np.mean([self.validation_loss[l] for l in r])),\n",
    "                  \"mean validataion accuracy: {}\".format(np.mean([self.validation_accuracy[l] for l in r])),\n",
    "                  \"mean testing loss: {}\".format(np.mean(self.test_cost)),\n",
    "                  \"mean testing accuracy: {}\".format(np.mean(self.test_acc)),\n",
    "                  \"mean testing % accuracy: {}\".format(np.mean(self.test_pct_acc))\n",
    "                 ]\n",
    "        csline = [str(layer_config), str(dropouts), str(feature_gen), \n",
    "                  'dr'+str(self.dropout)+'_lr'+str(self.learning_rate)+'_wd'+str(self.weight_decay), \n",
    "                  np.mean([self.train_loss[l] for l in r]), np.mean([self.train_accuracy[l] for l in r]), \n",
    "                  np.mean([self.validation_loss[l] for l in r]), np.mean([self.validation_accuracy[l] for l in r]), \n",
    "                  np.mean(self.test_cost), np.mean(self.test_acc), np.mean(self.test_pct_acc)]\n",
    "        \n",
    "        # if logging is on, send output there, otherwise print to console\n",
    "        if self.logfn:\n",
    "            with open(self.logfn, 'a') as f:\n",
    "                for p in prints:\n",
    "                    f.write(p)\n",
    "                    f.write('\\n')\n",
    "            with open(self.csvfn, 'a') as f:\n",
    "                wr = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONNUMERIC)\n",
    "                wr.writerow(csline)               \n",
    "        else:\n",
    "            for p in prints:\n",
    "                print(p)\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"\n",
    "        Plot training and validation loss and accuracy across epochs. Training must already be complete.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # Establish figure and axes\n",
    "        fig = plt.figure()\n",
    "        ax = plt.subplot(111)\n",
    "        # plot training and validataion loss and accuracy\n",
    "        plt.plot(self.train_accuracy[5:], label='train_accuracy')\n",
    "        plt.plot(self.train_loss[5:], label='train_loss')\n",
    "        plt.plot(self.validation_accuracy[5:], label='validation_accuracy')\n",
    "        plt.plot(self.validation_loss[5:], label='validaton_loss')\n",
    "        # Shrink current axis's height by 10% on the bottom\n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "                         box.width, box.height * 0.9])\n",
    "        # Put a legend below current axis\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "                  fancybox=True, shadow=True, ncol=5)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_errors(self):\n",
    "        plt.scatter([self.y_values[ti] for ti in self.test_range], self.test_acc)\n",
    "        plt.ylabel(\"Absolute error\")\n",
    "        plt.xlabel(\"MST value\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of MST estimation or indicate success when this file is imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data.\n",
      "Generating features.\n",
      "Starting optimization...\n",
      "Early stopping.\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAECCAYAAAASF7jxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt4FNXBx/Hv7iaB3EkIhGuIgNqKWlBrVRTW4h0pCEqF\nEqTam7eXom1VbAVvra34WmwrXoqACGjRtwqSoi2aVipFKYiCooLcDSAEQkLIbXPeP2Z3s9nsbjZA\nkt3M7/M888zMOWdmz0x2Z387M7sBEREREREREREREREREREREREREREREREREREREREREZEW1Rt4\nG9gIbAD+J0y7J4DPgfXAoNbpmoiIiEjL6gYM9E6nAZ8CXw9qcxVQ6J3+FvCf1umaiIiISOt6FRgW\nVPYU8N2A+U1Abqv1SEREJI4527oDErV8rMteq4PKewI7A+Z3Ab1aqU8iIiJxLaGtOyBRSQNeBiYD\n5SHqHUHzJrhBv379zJYtW1qgayIi7doWoH9bd0Jajs4Ixb5E4BXgBaxLY8F2Y91U7dPLW9bAli1b\nMMZoMIZp06a1eR9iZdC+0L7Qvog8AP1a4LguMURBKLY5gNnAx8Dvw7RZAkz0Tp8HHAL2tnzXRERE\n4p8ujcW2wcAE4ENgnbdsKpDnnX4a6xtjVwGbgSPA91u5jyIiInFLQSi2rSS6s3a3tXRH2hO3293W\nXYgZ2hf1tC/qaV+InQTfZCvtl/Fe725VNZ4aKmoq/MPR2qMcrTnK0dqjVNZWUlVbRWVtpTXtqaKq\ntooqTxXVnmr/UOOpscZ1NdR4aqxxXQ21dbXUeLxj77ynzmONjcc/7zGeBuM6U4fHWGPf4Cs3mAbl\nxpgG5cYYDKbR2LuD/WWAvzyUSH8Lh6Pxy9LhfakG1gWXBc6HqvOVN6cs2rHT4WxQBlhlUbQ91jLf\ndKjHCSzzTUdaX3Cb4LYh60/QfGDZ8bSLVB5qHYFlx7L+SOtqTnlT297WvH1o+45Ii9Ef1z6aHYRq\n62opOVrC/or97K/YT8nREg4ePUjJ0RIOVR6itKrUGipLKasuo6yqjLLqMsqryzlSfYTy6nI8xkNq\nYiopiSmkJKaQnJhMckIyHRM6kpxojTu4OvjHHRI6kORKooPLGie6Eung6kCiK5FEZyKJrkSr3JlI\ngjOBRJc1DhxcDhcup6vBdPDY6XDicrhwOBy4HK7GbwhBB+pwb6TBb/6hgkU4oQ7yof5GgUErXFlw\nGAuuCxXSmipr7tgXGH2P7ZsPbhsuUAa3j6adP7A28TiBwTbceppaZ3B5qPlQjxVqvqn1BQfxBmEc\ng6fOE/Jxg8ubWp9v3mM8EZcJtX7fMk31N7g88LED1xFqm3yvoRMZQn2v0Y9u/oj0DulNHgcVhNo/\nXRqzIWMMB44eYPuh7Wwv3c6uw7vYfXg3u8p2sad8D3vK97C3fC+HKg+RlZxFTkoOnZM70zmlM1kd\ns6whOYt+Wf3I7JhJRocM0pPSSe+Q7h+nJqaSmpRKB1eHmPhUFze0q0T8mhMemxNGjTGkJqW29eZJ\njNBh1z7MmJfG8HnJ52wp2UKiK5H8Tvn0yexDr4xe9EzvSc+MnnRP6063tG7kpuXSObkzLqerrfst\nItJmdEao/dMZIRu57rTrOLnzyf4zOSIiInanlGsfbXKztIhIPNMZofZPP6goIiIitqUgJCIiIral\nICQiIiK2pSAkIiIitqUgJCIiIralICQiIiK2pSAkIiIitqUgJCIiIralICQiIiK2pSAkIiIitqUg\nJCIiIralICQiIiK2pSAkIiIitqUgJCIiIralIBT7ngP2Ah+Fqc8BlgMfABuASa3TLRERkfinIBT7\n5gBXRKi/DVgHDATcwGNAQst3S0REJP4pCMW+d4CDEeqLgQzvdAZwAKht6U6JiIi0BzpzEP+eBd4C\nvgTSgbFt2x0REZH4oSAU/6Zi3R/kBvoBfwe+AZQFN5w+fbp/2u1243a7W6N/IiJxo6ioiKKiorbu\nhrQiR1t3QKKSDywFzghRVwg8DPzbO78CuAtYE9TOGGNaqn8iIu2Sw+EAvVe2a7pHKP5tAi7xTucC\npwJftF13RERE4odSbuxbBAzF+pr8XmAakOite9pbPgfIwwq2vwEWhliPzgiJiDSTzgi1f/rj2oeC\nkIhIMykItX+6NCYiIiK2pSAkIiIitqUgJCIiIralICQiIiK2pSAkIiIitqUgJCIiIralICQiIiK2\npSAkIiIitqUgJCIiIralICQiIiK2pSAkIiIitqUgJCIiIralICQiIiK2pSAkIiIitqUgJCIiIral\nICQiIiK2pSAkIiIitpXQ1h2Q1vOt//6XJKeTJIejwbhD0HySw0Gib+wtS/ROJ3iHRKfTP+0bXGCN\nAwfAGWHaGWbsCJh3BJU5vGUOwBHQxhFQR3PKHL4piYYxpn46sPxY24VYxkTZLtz6TJTtgqcDH/9Y\n25mgx21uOxNqmaDHC9en4Prm7NumtulEryO4LtTzp6ntDVfXVDuAS7OySHDqXIDUvxdI+2dWHTpE\njTFU1dVRYwzV3unqujr/dI0x/vlq77yvrNY7XWMMHmP887UB875pD+AxhrqAaY8x1AVMG/DX1wXN\nG29bX7mvro76A66v3AS1I0R9uLJwAoNSYJl/OiA8RWoXTXmgpvoVrl2k5SKFgXDriLYfELRfItVF\nuc8CA2o07YLX7atzRNmu0eM20d9o2oV8/jSjXci+B60nXJ+C65uzb5vaphO9juDtCbm+JrY33GM1\n1e7lAQNIdbloincZvVe2Y/rj2ocJ/vQpDUX8tHsMZzfCLd+UaM9QRRvAgusirT/aoCFiFwpC7Z8u\njcW+54DhwD7gjDBt3MDjQCKw3zsvzRTqk2dAZav2RUREWoeO7rHvIqAceJ7QQagT8G/gcmAXkIMV\nhoLpjJCISDPpjFD7pzvFYt87wMEI9eOBV7BCEIQOQSIiIhKCglD8OxnIBt4G1gAFbdsdERGR+KF7\nhOJfInAWMAxIAVYB/wE+D244ffp0/7Tb7cbtdrdKB0VE4kVRURFFRUVt3Q1pRbruGR/ygaWEvkfo\nLiAZmO6d/zOwHHg5qJ3uERIRaSbdI9T+6dJY/HsNuBBwYZ0R+hbwcZv2SEREJE7o0ljsWwQMxfo2\n2E5gGtblMICngU1YZ4A+BOqAZ1EQEhERiYpO99mHLo2JiDSTLo21f7o0JiIiIralICQiIiK2pSAk\nIiIitqUgJCIiIralICQiIiK2pSAkIiIitqUgJCIiIralICQiIiK2pSAkIiIitqUgJCIiIralICQi\nIiK2pSAkIiIitqUgJCIiIralICQiIiK2pSAkIiIitqUgJCIiIralICQiIiK2pSAkIiIitqUgJCIi\nIralIBT7ngP2Ah810e6bQC0wusV7JCIi0k4oCMW+OcAVTbRxAb8FlgOOFu+RiIhIO6EgFPveAQ42\n0eZ24GXgq5bvjoiISPuhIBT/egIjgVneedOGfREREYkrCW3dATluvwfuxgpADiJcGps+fbp/2u12\n43a7W7hrIiLxpaioiKKiorbuhrQi3U8SH/KBpcAZIeq+oP7vmANUAD8ElgS1M8boZJGISHM4HA7Q\ne2W7pjNC8a9vwPQcrMAUHIJEREQkBAWh2LcIGIp1tmcnMA1I9NY93VadEhERaQ90us8+dGlMRKSZ\ndGms/dO3xkRERMS2FIRERETEthSERERExLYUhERERMS2FIRERETEthSERERExLYUhERERMS2FIRE\nRETEthSERERExLYUhERERMS2FIRERETEthSERERExLYUhERERMS2FIRERETEthSERERExLYUhERE\nRMS2FIRERETEthSERERExLYUhERERMS2FIRERETEthSEYt9zwF7gozD13wPWAx8C/wbObKV+iYiI\nxD0Fodg3B7giQv0XwBCsAPQg8ExrdEpERKQ9UBCKfe8AByPUrwJKvdOrgV4t3iMREZF2QkGofbkJ\nKGzrToiIiMSLhLbugJwwFwM3AoPDNZg+fbp/2u1243a7W7xTIiLxpKioiKKiorbuhrQiR1t3QKKS\nDywFzghTfybwf1j3Em0O08YYY058z0RE2jGHwwF6r2zXdGks/uVhhaAJhA9BIiIiEoJSbuxbBAwF\ncrC+Rj8NSPTWPQ38GbgG2OEtqwHODbEenRESEWkmnRFq//THtQ8FIRGRZlIQav90aUxERERsS0FI\nREREbEtBSERERGxLQUhERERsS0FIREREbEtBSERERGxLQUhERERsS0FIREREbEtBSERERGxLQUhE\nRERsS0FIREREbEtBSERERGxLQUhERERsS0FIREREbEtBSERERGxLQUhERERsS0FIREREbEtBSERE\nRGxLQUhERERsS0FIREREbEtBKPY9B+wFPorQ5gngc2A9MKg1OiUiItIeKAjFvjnAFRHqrwL6AycD\nPwJmtUanRERE2gMFodj3DnAwQv13gHne6dVAJyC3pTslIiLSHigIxb+ewM6A+V1Arzbqi4iISFxJ\naOsOyAnhCJo3oRpNnz7dP+12u3G73S3XIxGROFRUVERRUVFbd0NaUfAbqMSmfGApcEaIuqeAIuBF\n7/wmYCjWDdaBjDEh85GIiIThcDhA75Xtmi6Nxb8lwETv9HnAIRqHIBEREQlBl8Zi3yKsMzw5WPcC\nTQMSvXVPA4VY3xzbDBwBvt8GfRQREYlLOt1nH7o0JiLSTLo01v7p0piIiIjYloKQiIiI2JaCkIiI\niNiWgpCIiIjYloKQiIiI2JaCkIiIiNiWgpCIiIjYloKQiIiI2JaCkIiIiNiWgpCIiIjYloKQiIiI\n2Jb+6arENWOgrq5+7BsCywOHUGXhBt/6fePgsuDppuqOlSPMfzkKVx6qLnDeN+1wRNcueJnmri/U\nss1tF+3jRlsmIuKjIGRzxkBVFZSXW8ORI1BRUT+urISjR61x4FBVBdXV9WPfUFNTP66pgdrayIPH\nYw2B03V1jad9ASdw2hc0fG9wLhc4nfXzgdOh5qMZfOsPfJzAsuDppuqO5e/TnPJQdaHCWWCwi9Qu\neJnmri9SoIy2XbSPG+2ygY4nTLV0WXBdU8tG+7xtql20j9uafQr3+OHmIy3rG8+bBykpiCgI2cnV\nV8OhQ1BaCocPQ1mZNXa5IC0NUlOtcUqKNZ2cXD907GiNO3Swho4doVMnSEpqOCQm1o99Q0KCNbhc\n9fMuV/04eNrpDD/tmw8MNSLNEW1wasuy4Lqmlo32TGZT7aJ93NbsU7jHDzcfadnAcWIiIgDobcQ+\nzJIlhk6dICMDMjMhPd0akpLaumsiIrHJYX3a0ntlO6Y/rn0Yc6JuWhERsQkFofZP3xoTERER21IQ\nEhEREdtSEBIRERHbUhCKfVcAm4DPgbtC1OcAy4EPgA3ApFbrmYiISJzTDWCxzQV8ClwC7AbeB8YB\nnwS0mQ50AO7BCkWfArlAbdC6dLO0iEgz6Wbp9k9nhGLbucBmYBtQA7wIjAxqUwxkeKczgAM0DkEi\nIiISgn5QMbb1BHYGzO8CvhXU5lngLeBLIB0Y2zpdExERiX8KQrEtmmtZU7HuD3ID/YC/A98AyoIb\nTp8+3T/tdrtxu90noIsiIu1HUVERRUVFbd0NaUW67hnbzsO6B+gK7/w9QB3w24A2hcDDwL+98yuw\nbqpeE7Qu3SMkItJMukeo/dM9QrFtDXAykA8kAd8FlgS12YR1MzVYN0mfCnzRSv0TERGJa7o0Fttq\ngduAN7C+QTYb6xtjP/bWPw38GpgDrMcKtr8ASlq9pyIiInFIp/vsQ5fGRESaSZfG2j9dGhMRERHb\nUhASERER21IQEhEREdtSEBIRERHbUhASERER21IQEhEREdtSEBIRERHbUhASERER21IQEhEREdtS\nEBIRERHb0v8as5N33oEOHaBjR0hOrh/7pl2utu6hiIhIq9L/T7EPYwYPhqoqqKyEo0frx77phIT6\nUNSxY31o8o2TkqzpDh2sad+QmFg/JCVZ6wksc7mssuDB5aqvc7nA6awvC54OHALLHI7G06HGzRmg\n8XS4suDpSHXB7UL/laKri7adb95XFmkdoeoClz2WdsGPeyztIvW9OcuGW9/xruN4yyLNH0u7cMuG\naxft+iKtI9rHOlH74ljLAsfTplnHtSbof421fzojZCcrV4avMwaqq61A5BuOHrWCky88VVfXz9fU\nWPO+wTdfUwO1tda4stKaDhw8nvp6j6d+vq6uft7jaThvjDXvKwuc9h3wAqd99YHzwQfpcINvX0Rz\nYA2ejlQX3C6SSIEp2mAVKpwda4gLXPZY2kUKlNG2i9T35iwbbn3Hu47jLYs0fyztwi0brl2064v2\nw8KxfKg4EdsYTVlwndiengn2of8+LyLSTDoj1P7pZmkRERGxLQUhERERsS0FIREREbEtBSERERGx\nLQUhERERsS0FIREREbEtBaHYdwWwCfgcuCtMGzewDtgAFLVKr+JYUVFRW3chZmhf1NO+qKd9IXai\nIBTbXMAfscLQacA44OtBbToBfwJGAKcD17ZmB+ORDvL1tC/qaV/U074QO1EQim3nApuBbUAN8CIw\nMqjNeOAVYJd3fn9rdU5ERCTeKQjFtp7AzoD5Xd6yQCcD2cDbwBqgoHW6JiIiEv/0s+GxbQzWZbEf\neucnAN8Cbg9o80fgLGAYkAKsAoZj3VMUaDPQryU7KyLSDm0B+rd1J6Tl6J+uxrbdQO+A+d7UXwLz\n2Yl1Oeyod/gX8A0aByG9kEVERCSuJGB9GskHkoAPaHyz9NeAf2DdWJ0CfIR1Y7WIiIhI3LsS+BTr\n0tY93rIfewefnwEbsULQ/7Rq70REREREREREJPZE84OM7VVvrG/TbcT6sUnf2bJs4O/AZ8CbWL/F\nZBcurB/fXOqdt+u+6AS8DHwCfIz1JQS77ot7qD+jvBDogH32xXPAXqxt94m07fdgHUs3AZe1Uh9F\n5Di4sC6p5QOJhL7HqD3rBgz0TqdhXWL8OvA74Bfe8ruAR1q/a23mDmABsMQ7b9d9MQ+40TudAGRi\nz32RD3yBFX4AXgJuwD774iJgEA2DULhtPw3rGJqItd82o5+gEYl55wPLA+bv9g529SpwCdanuVxv\nWTfvvB30wrqx/mLqzwjZcV9kYr35B7PjvsjG+oCQhRUIlwKXYq99kU/DIBRu2++h4Vn15cB5Ld05\naXlKs+1bND/IaBf5WJ/8VmMd5PZ6y/dSf9Br7x4Hfg7UBZTZcV+cBHwFzAHWAs8CqdhzX5QAjwE7\ngC+BQ1iXhey4L3zCbXsPGv58iZ2Pp+2KglD7Ztq6AzEiDevfkEwGyoLqDPbYT1cD+7DuDwr3Q6p2\n2RcJWD9C+qR3fITGZ0rtsi/6AT/F+qDQA+u1MiGojV32RShNbbtd90u7oiDUvkXzg4ztXSJWCJqP\ndWkMrE953bzT3bECQnt3AfAdYCuwCPg21j6x477Y5R3e986/jBWI9mC/fXEO8C5wAKgF/g/rkrod\n94VPuNdE8PG0l7dM4pyCUPu2But/keVj/SDjd6m/SdYOHMBsrG8F/T6gfAnWDaF4x6/S/k3FOoif\nBFwPvIX1f+nsuC/2YF0yPsU7fwnWt6aWYr99sQnrPpdkrNfLJVivFzvuC59wr4klWK+dJKzX0cnA\ne63eOxFptlA/yGgXF2LdD/MB1iWhdVg/J5CNddNwe/9qcDhDqQ/Edt0X38A6I7Qe6yxIJvbdF7+g\n/uvz87DOotplXyzCujeqGiscf5/I2z4V61i6Cbi8VXsqIiIiIiIiIiIiIiIiInJ8wn2N1s/lco1L\nSUl5vLy8vKsxpsn2IiIiIrHA6XTWpaenv1taWnoJUBWqTcRg43K5xqWlpc0uLCxMPuecc0hKSmqR\njoqIiIicaEePHmXUqFGVq1evnjNkyJDbli5dWhfcJmIQSk9P37N8+fLcCy64oOV6KSIiItJCtmzZ\nwsCBA2suvvjinwDzli5d6gmsj/g7QuXl5V3POeecFu2giIiISEvp06cPR44cScT6P4vu4PqIQcgY\n49DlMBEREYlXCQkJGGMADgN9g+v1y9IiIiJiB3VYPxjagK2D0M0338xDDz3U1t0QkTZQVFRE7971\n/zrq9NNP51//+ldUbZtLxxqR2BXXQSg/P5+33nrrmJefNWsWv/zlL09gj8RuTsQb3KRJk/jVr351\ngnokx2rDhg0MGTLkuNczd+5cLrroogZlOtZIS2jNMN8S3G43s2fPbutuxHcQcjgcvut+jdTW1rZy\nb9qGx+NpupGEFQth2uFw4HDoJ7okftnleBvrWjLMt4RYOfbFbRAqKChgx44djBgxgvT0dB599FGc\nTifPPfccffr04ZJLLgHguuuuo3v37nTq1ImhQ4fy8ccf+9cR+Em8qKiIXr168b//+7/k5ubSo0cP\n5s6d22Q/li1bxqBBg8jMzCQvL4/777+/Qf3KlSu54IILyMrKIi8vj3nz5gHWbxvceeed5Ofn06lT\nJy666CIqKytDpvbAN+vp06dz7bXXUlBQQGZmJvPmzeP999/n/PPPJysrix49enD77bdTU1PjX37j\nxo1ceumldO7cmW7duvHII4+wZ88eUlNTKSkp8bdbu3YtXbt2tVW4ipUwHa4P0rTf/va3XHfddQ3K\nJk+ezOTJk5k7dy6nnXYaGRkZ9OvXj2eeeSbsevLz81mxYgVgvT4nTZpEdnY2AwYM4P3332/Q9pFH\nHqF///5kZGQwYMAAXn3V+gfln3zyCTfffDOrVq0iPT2d7OxsoPFZv2effZaTTz6Zzp07M3LkSIqL\ni/11TqeTp59+mlNOOYWsrCxuu+22JvfBli1b+Pa3v01OTg5dunRhwoQJlJaW+ut37tzJ6NGj6dq1\nKzk5Odx+++0N+uLbRwMGDOCDDz7w9+OLL77wtwt1vPzd735H9+7duemmmzh06BBXX301Xbt2JTs7\nmxEjRrB7927/8iUlJXz/+9+nZ8+eZGdnM3r0aMA6i/H666/729XU1JCTk8P69eub3G6REyFug9D8\n+fPJy8vj9ddfp6ysjLFjxwLwr3/9i02bNvHGG28AMHz4cDZv3sxXX33FWWedxfe+9z3/OoLT6N69\nezl8+DBffvkls2fP5tZbb21wMAklLS2NF154gdLSUpYtW8asWbN47bXXANi+fTtXXXUVkydPZv/+\n/XzwwQcMHDgQgJ/97GesW7eOVatWUVJS4g9yoQQn5iVLlnDddddRWlrK+PHjcblczJw5kwMHDrBq\n1SpWrFjBk08+CUBZWRmXXHIJV111FcXFxWzevJlhw4bRrVs33G43f/nLXxrs03HjxuFyuaL6G8S7\nWAnTwSK9SU6ZMoXc3FwyMzM588wz2bhxIwCFhYUMGDCAjIwMevXqxWOPPXYceya+jBs3jsLCQsrL\nywHrLOnixYv53ve+R9euXVm2bBmHDx9mzpw5TJkyhXXr1oVcT+Dx4P7772fr1q188cUXvPHGG8yb\nN6/B67B///6sXLmSw4cPM23aNCZMmMDevXv5+te/zlNPPcX5559PWVmZ/4NG4Lrfeustpk6dyuLF\niykuLqZPnz5cf/31DfqybNky1qxZw4cffshf/vIX//EsknvvvZfi4mI++eQTdu7cyfTp0/374+qr\nr+akk05i+/bt7N692/94ixcv5v7772f+/PkcPnyYJUuW+MNbpP0D1vHy4MGD7Nixg6effpq6ujpu\nuukmduzYwY4dO0hOTm4Q4goKCqisrOTjjz9m3759TJkyBYAbbriBF154wd+usLCQnj178o1vfKPJ\nbW4v4iHMl5aWMnHiRLp27Up+fj4PP/yw/wPc3LlzufDCC/n5z39OdnY2ffv2Zfny5c3aB8YYHnro\nIfLz88nNzeWGG27g8OHDAFRWVjJhwgRycnLIysri3HPPZd++ff7H7tevHxkZGfTt25eFCxc263Gj\n7FtkcPzDscrPzzcrVqwwxhizdetW43A4zNatW8O2P3jwoHE4HObw4cPGGGMmTZpkfvnLXxpjjHn7\n7bdNcnKy8Xg8/vZdu3Y1q1evblafJk+ebKZMmWKMMebXv/61GT16dKM2Ho/HJCcnmw8//LBR3dtv\nv2169eoVdjunTZtmhg4dGrEPjz/+uLnmmmuMMcYsXLjQnHXWWSHbvfjii2bw4MHGGGNqa2tNt27d\nzPvvvx95A1sA0zkhw7EI3Lfbtm0zDofD3HDDDaaiosJUVlYaY4yZM2eOKS8vN9XV1eanP/2pGThw\noH/5SZMmmV/96lfGGOtvl5CQYKZNm2Zqa2tNYWGhSUlJMYcOHYrYh8Dn4YoVK0xOTo5Zt26dqaqq\nMrfffrsZMmSIMcaY5cuXm7PPPtuUlpYaY4zZtGmTKS4uNsYY061bN7Ny5UpjjDGHDh0ya9euPab9\ncVxOxMHgGA8IF154oXn++eeNMca8+eabpl+/fiHbjRo1ysycOdMY0/i1Fvhc6Nu3r3njjTf8dc88\n80yj12WggQMHmtdee80YYz1fLrzwwgb1gc+TG2+80dx1113+uvLycpOYmGi2b99ujDHG4XCYf//7\n3/76sWPHmkceeaSJPdDQX//6VzNo0CBjjDHvvvuu6dKlS4Njm89ll11mnnjiiZDrcDgcZsuWLQ22\nIfB4mZSUZKqqqsL2Yd26dSYrK8sYY8yXX35pnE5nyNfC7t27TVpamikrKzPGGDNmzBjz6KOPRrml\nJxZvv31Chubavn27SUlJ8e+D2tpa0717d7N69WqzbNky88UXXxhjjPnnP/9pUlJS/K/vSM/hu+66\nywwZMsQcPHjQ7Ny50wwYMMD07t3b33bx4sX+48dLL71kUlNTzZ49e4wxxsydO7fRc7igoMCMGjXK\nlJeXm23btplTTjnFzJ492xhjPecTExPNn//8Z1NXV2dmzZplevTo0eR2u91u/zpmz55t+vfvb7Zu\n3WrKy8vN6NGjTUFBgTHGmKeeesqMGDHCHD161NTV1Zm1a9eaw4cPm/LycpORkWE+++wzY4wxe/bs\nMRs3bgz5WIAZMWLEH0aMGHFLcNBJOP6kdLxrOLECLyvV1dUxdepUXn75Zb766iv/GZf9+/eTnp7e\naNnOnTt7xYSvAAAMRklEQVQ3OCuTkpLi/5QZzurVq7n77rvZuHEj1dXVVFVV+c9O7dy5k759G/1k\nAfv376eyspJ+/fod0zb26tWrwfxnn33GHXfcwX//+18qKiqora3F90OY4foAMHLkSG6++Wa2bdvG\npk2byMzMpC1+QNNMi40nkfE+madPn05ycrK/fNKkSf7padOmMXPmTMrKyvzPIRPwIkhMTOS+++7D\n6XRy5ZVXkpaWxqeffsq5554b8bF9n7QXLFjATTfd5D9z+Jvf/IasrCx27NhBUlISZWVlfPLJJ3zz\nm9/k1FNP9S+flJTExo0bOeOMM8jMzGTQoEHHtzOORRseDMaPH8+iRYsoKChg4cKF/jO/f/vb37j/\n/vv5/PPPqauro6KigjPPPLPJ9X355ZcNjiV5eXkN6p9//nkef/xxtm3bBkB5eTkHDhyIqq/FxcUN\nXmepqal07tyZ3bt3+x+nW7du/vpojkN79+5l8uTJrFy5krKyMurq6vyf5Hfu3EmfPn1CnnHetWvX\nMR+HunTp0uDfLlVUVDBlyhTeeOMNDh48CFj7xRjDzp07yc7OJjMzs9F6evToweDBg3n55ZcZNWoU\ny5cv5w9/+MMx9el4Gbe7TR43Ly+Ps846i7/+9a8UFBTw1ltvkZKS0ui4MWTIEC677DLeeeedJl/j\nixcvZtasWXTq1IlOnToxefJkHnjgAX/9tdde658eO3Ysv/nNb1i9ejXf+c53Gl2q93g8vPTSS6xf\nv57U1FRSU1O58847mT9/PjfeeCNg/WjhTTfdBMDEiRO55ZZb2LdvH127do1qHyxYsMB/uwhYx77T\nTz+dOXPmkJSUxIEDB/j8888544wz/Nt+5MgRnE4nH330Eb169SI3N5fc3NyoHi9Q3F4ag8aXjILL\nFixYwJIlS1ixYgWlpaVs3boVaPjGdbw3ao0fP55Ro0axa9cuDh06xE9+8hP/+vPy8tiyZUujZXJy\ncujYsSObN29uVJeamkpFRYV/3uPx8NVXX4XdRrC+uXTaaaexefNmSktLefjhh6mrq/P3IfA6f6CO\nHTty3XXX8cILL/DCCy8wceLE5m18OxUcpu+++2769+9PZmYmJ510EmCF2VCOJUwH8l0q8Ql8k7z4\n4ou57bbbuPXWW8nNzeXHP/4xZWVlALzyyisUFhaSn5+P2+3mP//5T7O2Od5de+21FBUVsXv3bl59\n9VXGjx9PVVUVY8aM4Re/+AX79u3j4MGDXHXVVVHdj9W9e3d27Njhnw+c3r59Oz/60Y/405/+RElJ\nCQcPHuT000/3r7epY0qPHj38AQqsg/mBAwfo2bNnM7e63tSpU3G5XGzYsIHS0lLmz5/vPwb07t2b\nHTt2hLz3r3fv3iGPQ2A9dwOPRcXFxQ22LXg7H3vsMT777DPee+89SktL+ec//4kxBmMMvXv3pqSk\nJOytBr7LY4sXL+aCCy6ge/fuzd4H8c4X5oFGYf68886jc+fOZGVlUVhYGFXojibMDxo0iKysLLKy\nstiwYUPY9e7fv5+ampoGx6a8vLwG94AFh3fguI59eXl51NbWsm/fPgoKCrj88su5/vrr6dmzJ3fd\ndRe1tbWkpqby0ksv8dRTT9GjRw+uvvpqPv3006gf0yeug1Bubm7IoOFTXl5Ohw4dyM7O5siRI0yd\nOrVBve9FejzKy8vJysoiKSmJ9957r8H1yfHjx/OPf/yDxYsXU1tby4EDB1i/fj1Op5Mbb7yRO+64\ng+LiYjweD6tWraK6uppTTjmFyspKCgsLqamp4aGHHqKqKuQ/zG3Qh/T0dFJSUti0aROzZs3y1w0f\nPpzi4mJmzpxJVVUVZWVlvPfee/76iRMnMmfOHJYsWUJBQcFx7Yt4FAthOlBTb5K33347a9as4eOP\nP+azzz7j0UcfBeCcc87h1Vdf5auvvmLUqFH+s5J20aVLF9xuN5MmTaJv376ceuqpVFdXU11dTU5O\nDk6nk7/97W+8+eabUa3P9wn50KFD7Nq1q8EZiiNHjuBwOMjJyaGuro45c+awYcMGf31ubi67du1q\n8IWFwGPNuHHjmDNnDuvXr6eqqoqpU6dy3nnnNXqjCly2KeXl5aSmppKRkcHu3bv9zwuAc889l+7d\nu3P33XdTUVFBZWUl7777LgA/+MEPmDFjBmvXrsUYw+bNm/2hb+DAgSxYsACPx8Py5cvDfi07sA/J\nyclkZmZSUlLS4Isj3bt358orr+SWW27h0KFD1NTUNFjfNddcw9q1a3niiSds+4EslsN8Tk4OiYmJ\nDY5NO3bsaHR14ngEH/t27NhBQkICubm5JCQkcN9997Fx40beffddXn/9dZ5//nkALrvsMt588032\n7NnD1772NX74wx82+7HjOgjdc889PPTQQ2RnZ/PKK680+uNNnDiRPn360LNnT04//XTOP//8Rp9o\nIn3CicaTTz7JfffdR0ZGBg8++CDf/e53/XV5eXkUFhby2GOP0blzZwYNGsSHH34IwIwZMzjjjDP4\n5je/SefOnbnnnnswxpCZmcmTTz7JD37wA3r16kVaWlqDVB/q64YzZsxg4cKFZGRk8KMf/Yjrr7/e\n3yY9PZ2///3vLF26lO7du3PKKadQVFTkX3bw4ME4nU7OPvvsmPuNidYQC2E62jfJNWvWsHr1ampq\nakhJSaFjx464XC5qampYsGABpaWluFwu0tPTbXPDe6Dx48ezYsUKxo8fD1jP/SeeeIKxY8eSnZ3N\nokWLGDlyZINlwr3mp02bRp8+fTjppJO44oormDhxor/taaedxp133sn5559Pt27d2LBhAxdeeKF/\n2WHDhjFgwAC6devmvywQ+LodNmwYDz74IGPGjKFHjx5s3bqVF198MWyfovmK8bRp01i7di2ZmZmM\nGDGCMWPG+JdxuVwsXbqUzZs3k5eXR+/evf1fkrj22mu59957GT9+PBkZGYwePdp/WWvmzJksXbqU\nrKwsFi5cyDXXXBNx3/30pz/l6NGj5OTkcMEFF3DllVc2aDN//nwSExP52te+Rm5uLk888YS/rmPH\njowePZpt27b5v01mN7Ec5l0uF2PHjuXee++lvLyc7du38/jjjzNhwoQTtv3jxo3zX24uLy9n6tSp\nXH/99TidToqKivjoo4/weDykp6eTmJiIy+Vi3759vPbaaxw5coTExERSU1Nb5NjX5M1OEv+GDRvm\nv2HNbl577TWTl5dnsrKyzIwZM4zT6WxwU2l5ebkZOXKkSU9PN/n5+eb55583TqfTfxNp8M3SgTcj\nGtPw5sVwAtdhjHVjYL9+/Ux2drYZMWKE2b17tzHGupH6zDPPNGlpaSYnJ8dMmDDBHDlyxFRXV5sr\nrrjCZGVlmYyMDHPuuec2uNlWJB488MAD/ptj7Wr+/PnG4XCYGTNm+Mv+9Kc/mdzcXNOpUydTUFBg\nxo0bF/aYE3i8qaioMBMnTjSdOnUyAwYMMI8++miDtvfee6/Jzs42OTk55o477mhw43J1dbUZPny4\nyc7ONl26dDHGWF82mjBhgunSpYvp3bu3efDBB01dXZ0xxrq5+qKLLmqwLYHHyXACH7Ours488MAD\npnfv3qZLly6moKDAf3P9okWLzKmnnmpSU1NNbm6umTx5svF4PKa4uNgMHTrUZGZmmk6dOpmLL77Y\nfPLJJyEfiwg3Szd1CsS7vLRX77//Ppdffjk7d+4kNTW1rbsjIjZUUlLC2Wefzfz58xucXRM5URwO\nByNGjPgj8MnSpUufDKyL60tjrWXAgAGkp6c3Gnw3tsWrG264gUsvvZTf//73CkEiMe4nP/lJyOPQ\nLbc0+oAbV5599lny8vK48sorFYKkTeiMkEgrGDBgQIObFX2eeeYZxo0b1wY9EhFpeWlpaSHvcVu+\nfDmDBw9utX5EOiMU8XeEHA6Hqa6udgT+VoSINJ/vF6BFROykOV+hbym1tbURv3AQ8dJYWlravjVr\n1pzwTomIiIi0hu3bt5Oamhr2d2giBqGKioopw4cPr3z33Xeprq4+8b0TERERaSFHjx7l1ltvrcnK\nylqDlXlqgttEvDTm8XgWVVdXJw8fPvyPpaWlybpfSEREROKF0+k0Xbp0KR40aFAR0Ado9K8Wmvxf\nYxUVFc8NGzbsP8BkwH6/0iYiIiLxrifwNlAUXBH1TymPGDEiE+iCwpCIiIjEl3KgeOnSpXVt3RER\nEREREREREREREREREREREWld/w92Vpzuj/kd+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117a37b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data.\n",
      "Generating features.\n",
      "Starting optimization...\n",
      "Early stopping.\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAECCAYAAAASF7jxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117538e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    n_nodes = 50\n",
    "    n_graphs = 100\n",
    "    m = MST_GCN(log=True)\n",
    "    m.train_model(n_graphs=n_graphs, n_nodes=n_nodes, train_frac=0.5, val_frac=0.3, \n",
    "                  feature_gen=['weighted_degree'],\n",
    "                  layer_config=[('gc', n_nodes, 'B'), ('d', 1, '')], \n",
    "                  dropouts=[False, False], test_n_nodes=int(n_nodes*0.8), verbose=False)\n",
    "    m.plot_results()\n",
    "    m.train_model(n_graphs=n_graphs, n_nodes=n_nodes, train_frac=0.5, val_frac=0.3, \n",
    "                  feature_gen=['weighted_degree'],\n",
    "                  layer_config=[('gc', n_nodes, 'B'), ('d', 1, '')], \n",
    "                  dropouts=[False, False], test_n_nodes=None, verbose=False)\n",
    "    m.plot_results()\n",
    "else:\n",
    "    print(\"MST_GCN imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
